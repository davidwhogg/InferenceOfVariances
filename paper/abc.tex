% This document is part of the InferenceOfVariance project.
% Copyright 2016 the author(s).

% To-do
% - finish first draft.
% - search for all occurrences of HOGG, DWH, or all-caps

\documentclass[12pt, letterpaper, preprint]{aastex}

% math shih
\newcommand{\setof}[1]{\left\{{#1}\right\}}

% text shih
\newcommand{\documentname}{note}
\newcommand{\equationname}{equation}

\begin{document}\sloppy\sloppypar\frenchspacing % oh yeah

\title{Inference of variances:\\
       Implications for cosmological large-scale structure experiments}
\author{
  David~W.~Hogg\altaffilmark{1,2}
}
\affil{Simons Center for Data Analysis,
       New York}
\altaffiltext{1}{also: Center for Cosmology and Particle Physics,
                       Department of Physics,
                       New York University}
\altaffiltext{2}{also: Center for Data Science,
                       New York University}
\email{david.hogg@nyu.edu}

\begin{abstract}
The study of large-scale structure is (at present) primarily the study
of variances, since the power spectrum or correlation function are
two-point statistics.
Present-day large-scale structure experiments often perform
cosmological inferences using a pseudo-likelihood that compares (by something like chi-squared) a
point estimate of the two-point function to an equivalent estimate
from a simulation or theory.
Here we show---with toy problems---that this procedure in general
produces incorrect posterior inferences about the variance (and
therefore cosmological parameters).
However, we also show that if the same pseudo-likelihood (or even something simpler) is used
\emph{not} as a likelihood in a standard probabilistic inference but
rather as a \emph{distance metric} in an approximate Bayesian
computation (ABC) method, correct posterior inferences can be drawn.
We discuss the relationships between the toy problems shown here and
the standard cosmological inferences, and try to suggest places where
a change from the standard incorrect method to a new method would
have the biggest impacts.
We argue that this is likely to be important at the baryon acoustic
scale and larger.
We emphasize that ABC is just one of many possible replacements for
the pseudo-likelihood method.
\end{abstract}

\keywords{
foo
---
bar
}

Present-day large-scale structure experiments are fundamentally
projects to measure the variance of cosmological fields, such as the
galaxy, quasar, intergalactic medium, or mass fields.
Indeed, if the initial conditions of the Universe are very close to
Gaussian (and they are) and if the evolution of the Universe is very
close to linear (as it is on large scales), then the mean and variance
of the density field (on some scale) are sufficient statistics to
infer everything about the initial conditions (at that scale).
Thus, the measurement or inference of variance as a function of
scale---or, equivalently, inference of the power spectrum or the
two-point auto-correlation function---is critical to inference of the
cosmological initial conditions and the cosmological parameters.
There is an enormous literature on making these cosmological
measurements!
Here we won't address this literature in detail.
We are going to step back and look at issues of principle in measuring
variances, and see if there are small changes to be made to
present-day cosmological practice that might improve future cosmological
inferences.

Imagine \emph{the most trivial variance estimation problem possible}:
You have been given $N$ samples $y_n$ drawn fairly and independently (iid)
from a Gaussain pdf.
What is the mean $\mu$ and variance $V$ of the pdf from which they
were drawn?
There is a classical answer that the empirial mean and empirical
variance of the sampling
\begin{eqnarray}
\bar{y} &\equiv& \frac{1}{N}\,\sum_{n=1}^N y_n
\label{eq:empmean}\\
\sigma^2_y &\equiv& \frac{1}{N-1}\,\sum_{n=1}^N [y_n - \bar{y}]^2
\label{eq:empvar}
\end{eqnarray}
(with possibly some tiny adjustments) are good
\emph{estimators}\footnote{There is a little bit of confusion in the
  cosmological literature between the variance of a distribution and
  the empirical variance of a sampling of that distribution. In this
  \documentname, the \emph{mean} and \emph{variance} are properties of
  the pdf from which the data were generated. The \emph{empirical
    mean} and the \emph{empirical variance} are things you get by
  doing arithmetical operations like \equationname
  s~(\ref{eq:empmean}) and (\ref{eq:empvar}) on the data.  This
  distinction is important for cosmology, because sometimes there are
  statements or implications that the cosmological parameters are
  being inferred from the variance of the density field. They are not!
  They are being inferred from an \emph{empirical estimate} of the
  variance of the density field. And while these empirical estimates
  are sometimes sufficient statistics for inference of cosmological
  parameters (as we note above), this is only relevant and valuable if
  they are used \emph{correctly}.} for the mean and variance of the
pdf from which the $y_n$ were drawn.
That's cool, but what if we want full posterior information about the
mean $\mu$ and variance $V$?
In contemporary cosmological experiments we \emph{always} want full
posterior (or at least likelihood) information about the parameters.
That is, the cosmological inference community is (mostly) Bayesian,
so in what follows we will produce Bayesian outputs, which are (for
our purposes) likelihood functions, posterior pdfs, and posterior
samplings.

The (Bayesian) Right Thing To Do is to write down our beliefs about
how the data were generated, and then derive (analytically or
numerically) the posterior pdf\footnote{There will be several possible
  points of confusion in this \documentname. One is that there is a
  pdf for the data $y_n$. And there is also a pdf for the parameters
  $(\mu,V)$. These are different pdfs that live in different
  spaces. Another is that the data $y_n$ were drawn from a Gaussian,
  while the parameters $(\mu,V)$ will (in general) not have a
  posterior pdf that is Gaussian. That is, even if the data are
  generated by a Gaussian, there is no sense that the likelihood
  function for the parameter $V$ will be Gaussian.} for the mean and
variance.
This, however, is \emph{not what's typically done in a cosmological
experiment!}
The reason is simple: It is difficult or perhaps impossible to compute
the likelihood in the cosmological context: It would require a pdf
over all possible experimental outcomes, which means all possible
positions of all possible galaxies in the survey volume.\footnote{We
  believe that there might be a tractable likelihood function for at
  least some cosmological experiments. We are working on testing this
  now. HOGG CITE WANDELT and related.}
What's done is to compare the empirical variance of the observed data
to the variance observed in mock data or a simulation, and find
parameters that generate simulations that generate similar observed
variances.
There is nothing wrong with this procedure in general.
There are a large family of methods by which this comparison could be
done that would generate correct inferences about the cosmological
parameters.
However, the cosmological community has not (generally) used a method
that can be shown to generate correct inferences.

% HOGG: SYNTACTICAL REFERENCE

HOGG: Introduce the Gaussian pseudo-likelihood. Why is this patently absurd?

We have emphasized that a critical property of the variance of the Gaussian (or any other
distribution) is that it is a \emph{non-negative} quantity.
How does this relate to the properties of a two-point correlation
function (for, say, galaxies, the temperature map of the cosmic
microwave background, or the initial conditions of the density field)?
The answer is slightly non-trivial:
It is that the correlation function (what ought to be called the
``covariance function'') be a non-negative semi-definite function, or
that the correlation function obey what's called Mercer's condition\footnote{HOGG: REFERENCE OR LINK},
or that any covariance matrix made up by any evaluation of the
correlation function on any grid of points be non-negative
semi-definite.
These (identical) conditions are best explained with equations.

First, define the correlation function $\xi(|r|)$ as the covariance of
the overdensity field $\delta(x)$, where $r$ is a vector displacement
in three-space (and $|r|$ is a scalar separation) and $x$ is
three-dimensional position:
\begin{eqnarray}
\xi(|r|) &=& E[\delta(x)\,\delta(x+r)]
\\
\delta(x) &\equiv& \frac{\rho(x)}{\bar{\rho}} - 1
\quad ,
\end{eqnarray}
where $E[q]$ is the expectation value of $q$ (implicitly taken as an integral
over all of space),
$\rho$ is the density field,
and $\bar{\rho}$ is the mean density.
By assuming that $\xi(\cdot)$ depends only on $|r|$ we are effectively
assuming statistical isotropy.
Now choose a spatial set of points $\setof{x_i}$ in three-dimensional space
and construct the matrix $C$ such that the elements $C_{ij}$ of $C$
are given by
\begin{eqnarray}
C_{ij} &\equiv& \xi(|x_i - x_j|)
\quad .
\end{eqnarray}
This matrix is manifestly symmetric, but if $\xi(|r|)$ is the true
covariance function of the density field, then this matrix must also
be non-negative semi-definite.
That is, it must have eigenvalues that are real and non-negative.
That this be true for any choice of points $\setof{x_i}$ is equivalent
(for our purposes) to Mercer's condition.
That is, there is no requirement that the correlation function (really
covariance function) be positive everywhere; it can go negative!
But it must be such that it always produces non-negative semi-definite
covariance tensors.

HOGG: Return to the point about the difference between the empirical
correlation function estimate and the true correlation function. Only the
latter need obey Mercer's condition. That's part of the problem!

\acknowledgements
It is a pleasure to thank
  Brendon~J.~Brewer (Auckland),
  Chang~Hoon~Hahn (NYU),
  Dan~Foreman-Mackey (UW),
  M.~J.~Vakili (NYU),
  and the Blanton--Hogg group meeting at NYU
for valuable discussions.

\end{document}
