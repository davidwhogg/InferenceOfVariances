% This document is part of the InferenceOfVariance project.
% Copyright 2016 the author(s).

% To-do
% - get smarter about terminology between observation and theory and simulation etc.
% - make a comment somewhere (early) about accuracy vs precision.
% - make absolutely clear what is speculation and what is demonstrated.
% - finish first draft.
% - cite Percival and Eisenstein. Be self-deprecating.
% - cite ABC papers like Cisewski, Cameron.
% - typeset projects properly (see acks)
% - new AAS \software tags
% - search for all occurrences of HOGG, DWH, or all-caps

\documentclass[12pt, letterpaper, preprint]{aastex}
\usepackage{hyperref}
\input{vc}
\input{data_0005}

% typesetting shih
\linespread{1.08} % close to 10/13 spacing
\setlength{\parindent}{1.08\baselineskip} % Bringhurst
\setlength{\parskip}{0ex}
\let\oldbibliography\thebibliography % killin' me.
\renewcommand{\thebibliography}[1]{%
  \oldbibliography{#1}%
  \setlength{\itemsep}{0pt}%
  \setlength{\parsep}{0pt}%
  \setlength{\parskip}{0pt}%
  \setlength{\bibsep}{0ex}
  \raggedright
}
\setlength{\footnotesep}{0ex} % seriously?

% math shih
\newcommand{\setof}[1]{\left\{{#1}\right\}}
\newcommand{\given}{\,|\,}
\newcommand{\pseudo}{{\mathrm{pseudo}}}
\newcommand{\Var}{\mathrm{Var}}

% text shih
\newcommand{\foreign}[1]{\textsl{#1}}
\newcommand{\etal}{\foreign{et~al.}}
\newcommand{\opcit}{\foreign{Op.~cit.}}
\newcommand{\documentname}{\textsl{Article}}
\newcommand{\equationname}{equation}

\begin{document}\sloppy\sloppypar\frenchspacing % oh yeah

\title{Inference of variances:\\
       Implications for cosmological large-scale structure experiments}
\author{
  David~W.~Hogg\altaffilmark{1,2}
}
\affil{Simons Center for Data Analysis,
       New York}
\altaffiltext{1}{also: Center for Cosmology and Particle Physics,
                       Department of Physics,
                       New York University}
\altaffiltext{2}{also: Center for Data Science,
                       New York University}
\email{david.hogg@nyu.edu}

\begin{abstract}
The study of large-scale structure is (at present) primarily the study
of variances, since the power spectrum or correlation function are
two-point statistics.
Present-day large-scale structure experiments often perform
cosmological inferences using a pseudo-likelihood that compares
(by something like chi-squared) a
point estimate of the two-point function to an equivalent estimate
from a simulation or theory.
Here we show---with toy problems---that this procedure in general
produces incorrect posterior inferences about the variance (and
therefore cosmological parameters).
However, we also show that if a very similar pseudo-likelihood
is used
\emph{not} as a likelihood in a standard probabilistic inference but
rather as a \emph{distance metric} in a likelihood-free inference or
approximate Bayesian
computation (ABC) method, correct posterior inferences can be drawn.
We discuss the relationships between the toy problems shown here and
the standard cosmological inferences, and try to suggest places where
a change from the standard incorrect method to a new method would
have the biggest impacts.
We argue that this is likely to be important at the baryon acoustic
scale and larger, and it is likely to lead to underestimates of
amplitude parameters such as sigma-8.
We emphasize that ABC is just one of many possible replacements for
the pseudo-likelihood method.
\end{abstract}

\keywords{
methods: data analysis
---
methods: statistical
---
galaxies: statistics
---
cosmological parameters
---
cosmology: observations
---
large-scale structure of universe
}

Present-day large-scale structure experiments are fundamentally
projects to measure the variance of cosmological fields, such as the
galaxy, quasar, intergalactic medium, or mass fields.
Indeed, if the initial conditions of the Universe are very close to
Gaussian (and they are) and if the evolution of the Universe is very
close to linear (as it is on large scales), then the mean and variance
of the density field (on some scale) are sufficient statistics to
infer everything about the initial conditions (at that scale).
Thus, the measurement or inference of variance as a function of
scale---or, equivalently, inference of the power spectrum or the
two-point auto-correlation function---is critical to inference of the
cosmological initial conditions and the cosmological parameters.
There is an enormous literature on making these cosmological
measurements!
Here we won't address this literature in detail.
We are going to step back and look at issues of principle in measuring
variances, and see if there are small changes to be made to
present-day cosmological practice that might improve future cosmological
inferences.

Imagine \emph{the most trivial variance estimation problem possible}:
You have been given $N$ samples $y_n$ drawn fairly and independently (iid)
from a Gaussian probability density function (hereafter ``pdf'').
What is the mean $\mu$ and variance $V$ of the pdf from which they
were drawn?
There is a classical answer that the empirial mean and empirical
variance (or sample mean and sample variance) of the sampling
\begin{eqnarray}
\bar{y} &\equiv& \frac{1}{N}\,\sum_{n=1}^N y_n
\label{eq:empmean}\\
s^2_y &\equiv& \frac{1}{N-1}\,\sum_{n=1}^N [y_n - \bar{y}]^2
\label{eq:empvar}
\end{eqnarray}
(with possibly some tiny adjustments) are good
\emph{estimators}\footnote{There is a little bit of confusion in the
  cosmological literature between the variance of a distribution and
  the \emph{empirical variance of a sampling} of that distribution. In this
  \documentname, the \emph{mean} and \emph{variance} are properties of
  the pdf from which the data were generated. The \emph{empirical
    mean} and the \emph{empirical variance} are things you get by
  doing arithmetical operations like \equationname
  s~(\ref{eq:empmean}) and (\ref{eq:empvar}) on the data.  This
  distinction is important for cosmology, because sometimes there are
  statements or implications that the cosmological parameters are
  being inferred from the variance of the density field. They are not!
  They are being inferred from an \emph{empirical estimate} of the
  variance of the density field---a sample variance. And while these empirical estimates
  will be sufficient statistics for the variance,
  this is only relevant and valuable if
  they are used \emph{correctly}.} for the mean and variance of the
pdf from which the $y_n$ were drawn.
That's cool, but what if we want full posterior information about the
variance $V$?
In contemporary cosmological experiments we \emph{always} want full
posterior (or at least likelihood) information about the parameters.
That is, the cosmological inference community is (mostly) Bayesian,
so in what follows we will produce Bayesian outputs, which are (for
our purposes) likelihood functions, posterior pdfs, and posterior
samplings.

It will be important for what follows that while equations
(\ref{eq:empmean}) and (\ref{eq:empvar}) deliver only empirical
point estimates of the mean and variance of the pdf we care about,
they \emph{are} sufficient statistics for the inference!
That is, \emph{if we believe that the $y_n$ were drawn iid from a
  Gaussian}, it is possible to write down a correct likelihood
function for the set $\setof{y_n}_{n=1}^N$ using only these two
summary statistics.
As we will see, however, it is also possible to write down an
incorrect likelihood function using them!

From here on, we will name the problem of inferring cosmological
parameters from a large-scale structure survey or cosmic microwave
background measurement the ``real problem'', and we will name the
problem of finding the variance $V$ of the one-dimensional Gaussian
that generated the $y_n$ the ``trivial problem''.
That doesn't make the trivial problem trivial.
It is relatively subtle, as we hope you will agree by the end of this!

The (Bayesian) Right Thing To Do in the trivial problem is to write
down our beliefs about how the data were generated, and then derive
(analytically or numerically) the posterior pdf\footnote{There will be
  several possible points of confusion in this \documentname. One is
  that there is a pdf for the data $y_n$. And there is also a pdf for
  the parameters $(\mu,V)$. These are different pdfs that live in
  different spaces. Another is that the data $y_n$ were drawn from a
  Gaussian, while the parameter $V$ will (in general) not have a
  posterior pdf or likelihood function that is Gaussian in shape. That
  is, even if the data are generated by a Gaussian, there is no sense
  that the likelihood function will be Gaussian.} for the mean and
variance.
This, however, is \emph{not what's typically done in a cosmological
experiment!}
The reason is simple: It is difficult or perhaps impossible to compute
the likelihood in the cosmological context: It would require a pdf
over all possible experimental outcomes, which means all possible
positions of all possible galaxies in the survey volume.\footnote{We
  believe that there might be a tractable likelihood function for at
  least some cosmological experiments. We are working on testing this
  now. (Don't hold your breath.)}
What's done instead is to compare the empirical variance (the
correlation function or the power spectrum) of the observed data to
the variance observed in mock data or a simulation, and find
parameters that generate simulations that generate similar observed
variances.
There is nothing wrong with this procedure in general.  There are a
large family of methods by which this comparison could be performed
that would generate correct inferences about the cosmological
parameters.
However, the cosmological community has not (generally) used any of
these methods.

Now in the trivial problem---infer the variance $V$ given sampling
$\setof{y_n}_{n=1}^N$---one approach one could take to developing full
posterior inferences about $V$ would be the following:
Since the empirical variance $s^2_y$ defined in (\ref{eq:empvar})
is a sufficient statistic for the process variance $V$, \emph{make up}
a \emph{pseudo-likelihood} $p_\pseudo(s^2_y\given V)$ of the form
\begin{eqnarray}
\ln p_\pseudo(s^2_y\given V) &=& -\frac{1}{2}\,\frac{[s^2_y - V]^2}{\Sigma^2}
\label{eq:pseudo}\quad ,
\end{eqnarray}
where $\Sigma^2$ is a variance on the variance, determined somehow (to
be discussed more below).
This pseudo-likelihood gets the prefix ``pseudo'' because it was
created by an ansatz, and not by writing down our beliefs about the
process by which the samples were generated.
It gets the suffix ``likelihood'' because it is a (made up)
probability for the data given parameters, where in this case
$s^2_y$ is a sufficient statistic of the data, and $V$ is a
parameter of the unknown pdf from which the samples were drawn.
Note the Bayesianism inherent here:
The \emph{likelihood} is going to be the critical component of the
inference; the likelihood is based on \emph{beliefs} about the
data-generation process; and the parameter $V$ of the pdf that
generates the data is unknown (and, essentially, unknowable).

This made-up pseudo-likelihood (\ref{eq:pseudo}) for the trivial
problem is patently absurd: For one, it does not represent anyone's
beliefs about the data generation.
For another, it has no pathology or odd behavior when the variance $V$
goes to zero or negative!
That is, it doesn't even respect the basic properties of variances.
And yet, this is \emph{precisely analogous to the standard practice in
  large-scale structure}.

What's done in the real problem is the following:
Observations are made of galaxies, quasars, lyman-alpha clouds, CMB
temperature temperature fluctuations, or other tracer.
From these observations, a set of empirical variances are constructed, in the
form of a two-point correlation function or power
spectrum.\footnote{There is a lot of science about how to construct
  these estimator or measurements of two-point statistics. That's
  beyond our scope here. The point here is just that they \emph{are}
  constructed.}
This leads to some set of $M$ variance measurements $q_m$ (of, say, the
empirical correlation function in radial separation bins, or the
angular power spectrum in spherical-harmonic orderbins, or the power
spectrum in a spatial-scale bin.)
These variance measurements $q_m$ are compared to a theoretical model (which makes
predictions $\xi(r_m;\theta)$ for the measurements $q_m$) by a
pseudo-likelihood that looks like a $\chi^2$:
\begin{eqnarray}
\ln p_\pseudo(\setof{q_m}_{m=1}^M\given\theta) &=&
 -\frac{1}{2}\,\sum_{m=1}^M\sum_{m'=1}^M [q_m - \xi(r_m;\theta)]\,\Sigma^{-2}_{mm'}\,[q_{m'} - \xi(r_{m'};\theta)]
\label{eq:pseudoreal}\quad ,
\end{eqnarray}
where the $\Sigma^{-2}_{mm'}$ are the elements of an inverse
covariance matrix, computed from the theory, as discussed a bit more
below, and the theoretical predictions $\xi(r_m;\theta)$ are a
function of the cosmological parameters $\theta$ that we are trying to
infer (that is, for which we want posterior samplings).\footnote{There
  are a lot of things in this presentation that imply that we are
  thinking about real-space three-dimensional correlation functions
  (which we are), but the discussion applies to many other kinds of
  experiments with essentially no adjustment.}
This pseudo-likelihood may seem sensible---and huge resources are put
into computing good values for the $\Sigma^{-2}_{mm'}$\footnote{For
  example, \cite{percival}.}---but it is patently wrong, in the same
way that the trivial-problem pseudo-likelihood (\ref{eq:pseudo}) is
wrong.

Since the non-negativity point---the point that nothing about the trivial-problem
pseudo-likelihood in (\ref{eq:pseudo}) goes wrong when the variance
$V$ goes near or below zero---is such a killing argument in the
trivial problem, let's reflect for a moment on how this non-negativity
point comes in to the real problem of large-scale structure
inferences.
What is the equivalent property to non-negativity for the correlation
function (for, say, galaxies, the temperature map of the cosmic
microwave background, or the initial conditions of the density field)?
The answer is slightly non-trivial:
It is that the correlation function (what ought to be called the
``covariance function'') be a non-negative semi-definite function, or
that the correlation function obey what's called Mercer's condition\footnote{\url{https://en.wikipedia.org/wiki/Mercer\%27s_condition}}
or that any covariance matrix made up by any evaluation of the
correlation function on any grid of points be non-negative
semi-definite.
These (identical) conditions are best explained with equations.

First, define the correlation function $\xi(|r|)$ as the covariance of
the overdensity field $\delta(x)$, where $r$ is a vector displacement
in three-space (and $|r|$ is a scalar separation) and $x$ is
three-dimensional vector position:
\begin{eqnarray}
\xi(|r|) &=& E[\delta(x)\,\delta(x+r)]
\\
\delta(x) &\equiv& \frac{\rho(x)}{\bar{\rho}} - 1
\quad ,
\end{eqnarray}
where $E[q]$ is the expectation value of $q$ (implicitly taken as an integral
over all possible draws of the density field $\rho(x)$ and over all space in each of those draws),
$\rho$ is the density field,
and $\bar{\rho}$ is the mean density.
By assuming that $\xi(\cdot)$ depends only on the magnitude $|r|$ we are effectively
assuming statistical isotropy.
Now choose a spatial set of points $\setof{x_i}$ in three-dimensional space
and construct the matrix $C$ such that the elements $C_{ij}$ of $C$
are given by
\begin{eqnarray}
C_{ij} &\equiv& \xi(|x_i - x_j|)
\quad .
\end{eqnarray}
This matrix is manifestly symmetric, but if $\xi(|r|)$ is the true
covariance function of the density field, then this matrix must also
be non-negative semi-definite.
That is, it must have eigenvalues that are real and non-negative.
That this be true for any choice of points $\setof{x_i}$ is equivalent
(for our purposes) to Mercer's condition.
That is, there is no requirement that the correlation function (really
covariance function) be positive everywhere; it can go negative!
But it must be such that it always produces non-negative semi-definite
covariance tensors.\footnote{This has an important consequence for
  inferences we do in cosmology: \emph{When we infer the correlation
    function (what should be called the covariance function) of the
    density field, let's only consider functions that obey Mercer's
    condition!}  (After all, we would never consider $V<0$ in the
  trivial problem!)  This is much easier to do in Fourier space than
  real space.  That isn't an argument for doing the \emph{analysis} in
  Fourier space, but it is an argument for representing the function
  $\xi(|r|)$ in Fourier space. This is off-topic for the present
  \documentname, of course, since this is a point about the
  \emph{prior} and not about the \emph{likelihood}. But somehow it
  feels relevant. Of course if the inference is being done of the
  cosmological parameters (and not $\xi(|r|)$ itself), then only
  functions obeying Mercer's condition are under consideration,
  because the theory itself can only produce non-negative
  semi-definite functions.}
Continuing the analogy with the trivial problem, in the real problem,
one of the things that is clearly wrong with the pseudo-likelihood
(\ref{eq:pseudoreal}) is that nothing goes strange or bad or singular as
the function $\xi(r;\theta)$ starts to violate Mercer's condition.

Now back to the trivial problem of determining the variance $V$ of a
one-dimensional Gaussian from samples:
What exactly would be the Right Thing To Do in this problem?
It is very straightforward:
We write down the probability for each point, and product these
(or sum them in the log).
For iid $y_n$ drawn from a Gaussian, this looks like this:
\begin{eqnarray}
p(\setof{y_n}_{n=1}^N\given \mu,V) &=& -\frac{1}{2}\,\sum_{n=1}^N \frac{[y_n - \mu]^2}{V} - \frac{N}{2}\,\ln V
\label{eq:truelf}\quad ,
\end{eqnarray}
which is obtained by taking the ln of a Gaussian function that has
mean $\mu$ and variance $V$.
It is left as an exercise to the reader to show that this likelihood
can be expressed in terms of (only) the two sufficient statistics given
in equations~(\ref{eq:empmean}) and (\ref{eq:empvar}).\footnote{Hint:
  Equation~(\ref{eq:truelf}) is quadratic in the data.}
Below, we will discuss the Right Thing To Do in the large-scale
structure problem. It's hard!

In order to test these ideas quantitatively, we need fake data.
Here we choose (arbitrarily) a True\footnote{Because we are making
  fake data, we have access to God's Truth (with a capital
  ``T''). This is not the case for real astronomical measurements!}
mean $\mu=7.0$ and True variance $V=17.0$ (units arbitrary).
With code associated with this \documentname\footnote{The calculations
  and \figurename s in this \documentname\ were generated from the code in
  repository \giturl\ (with hash \texttt{\githash~\gitdate}). You can
  clone this and run the code yourself!}, we generate $N=5$ samples
$y_n$.
These samples (and their empirical moments) are
\begin{eqnarray}
\setof{y_n}_{n=1}^N &=& \setof{\samples}
\label{eq:data}\\
\bar{y} &=& \samplemean
\\
s^2_y &=& \samplevar
\quad .
\end{eqnarray}

Now imagine (without questioning) that our prior pdf for the
parameters $\mu, V$ is uniform in the joint interval $0<\mu<10$ and
$0<V<100$.
At this stage, we have two options for inference: Use the correct
likelihood (\ref{eq:truelf}) or else the pseudo-likelihood
(\ref{eq:pseudo}).
We do both, running a standard (but home-built) Metropolis--Hastings
MCMC code for $524288$ steps, and thinning the chain by a factor of
$16$.
A Gaussian in each parameter $\mu,V$ was employed as a proposal
distribution, and step sizes were set to give reasonable acceptance.
The details are all visible in the code\footnote{\opcit} associated
with this \documentname.
The results are shown in \figurename~\ref{fig:correct} and
\figurename~\ref{fig:pseudo}.%
\begin{figure}
\includegraphics{correct_0005.png}
\caption{A posterior sampling for the mean $\mu$ and variance $V$,
  given the data in (\ref{eq:data}), flat prior in the displayed
  parameter space, and the (correct, appropriate) likelihood given in
  (\ref{eq:truelf}). Note that the data are not consistent with very
  small variances but they \emph{are} consistent with very large
  variances.\label{fig:correct}}
\end{figure}%
\begin{figure}
\includegraphics{pseudo_0005.png}
\caption{Same as \figurename~\ref{fig:correct} but with the likelihood
  replaced with the pseudo-likelihood (\ref{eq:pseudo}). The
  pseudo-likelihood does not involve the mean $\mu$, so it remains
  unconstrained and the marginalized posterior pdf matches the prior
  pdf for this parameter. The posterior on the variance $V$ looks very
  different from the correct answer shown in
  \figurename~\ref{fig:correct}.\label{fig:pseudo}}
\end{figure}

Two comments on these results:
First, the two inferences deliver very different results for the
variance $V$. This is the most important point.
In the pseudo-likelihood inference (\figurename~\ref{fig:pseudo}), the
pdf mean $\mu$ is unconstrained. This is because $\mu$ doesn't appear
at all in the pseudo-likelihood! But the key point is that the
posterior inferences about $V$ are very different, especially at very
small and very large values of $V$.
In particular, the pseudo-likelihood permits low variances that ought
to be (truly) ruled out, and rules out high variances that are (truly)
permitted. This might mean that large-scale structure projects using
analogous methods might be biased low in power-spectrum amplitude
parameters like $\sigma_8$.

\textbf{Take-home message 1:} \emph{The standard practice in
  cosmology, even when applied to an extremely trivial problem,
  delivers incorrect inferences.}

Second, the use of the pseudo-likelihood (\ref{eq:pseudo}) required
the setting of the variance-of-variance parameter $\Sigma^2$.
To set this, we made use of theory\footnote{\url{https://en.wikipedia.org/wiki/Variance}}
about the variance $\Var[s^2_y]$ of
the empirical sample variance $s^2_y$.
\begin{eqnarray}
\Var[s^2_y] &=& \frac{1}{N}\,\left[\mu_4 - \frac{N-3}{N-1}\,V^2\right]
\quad ,
\end{eqnarray}
where $\mu_4$ is the (true, not estimated) fourth moment of the pdf $p(y)$.
For a Gaussian pdf this becomes
\begin{eqnarray}
\Var[s^2_y] &=& \frac{2\,V^2}{N-1}
\quad .
\end{eqnarray}
For these reasons, we set the pseudo-likelihood parameter $\Sigma^2$ to
\begin{eqnarray}
\Sigma^2 &\leftarrow& \frac{2\,[s^2_y]^2}{N-1}
\quad ,
\end{eqnarray}
where we are not being conservative because, for one, we are imagining
that $s^2_y$ is a good estimator for $V$, and, for two, we are
assuming implicitly that the true distribution has vanishing kurtosis
(is nearly Gaussian).
Although this choice is not conservative, it maps directly onto the
practice in large-scale structure studies, which is to set the
equivalent parameter (which in the real problem is a tensor or matrix)
using simulations of the data-generating process with parameters set
to something estimated from the data.\footnote{See, for example, \cite{percival}.}
That is, the pseudo-likelihoods used in cosmology are not conservative
in exactly this sense.
It is important to note, however, that there is no choice that we
could make for $\Sigma^2$ such that our posterior inferences would e
correct given this form for the pseudo-likelihood; that is, the method
is not wrong because it is not conservative. It is wrong because it
is using an unreasonable likelihood function.
In particular, the biases---that the pseudo-likelihood permits low
variances that ought to be ruled out, and that the pseudo-likelihood
rules out high variances that are in fact acceptable---exist for any
choice of the variance-of-variance parameter $\Sigma^2$.

In the real problem of large-scale structure, it is intractable (probably) to 
write down the true likelihood.
The pseudo-likelihood gives wrong inferences.
What is a cosmologist to do?
We can simulate the Universe, but we can't write down a a likelihood.
There is a large class of likelihood-free solutions, in which a model
or simulation is used to generate fake data, and the posterior pdf can
be sampled by carefully constructed comparison of simulated data to
the real data.
The simplest of these likelihood-free is approximate Bayesian
computation (ABC)\footnote{ABC has been used in astronomy in HOGG, and
  HOGG.}, a simple version of which we introduce now.

The idea behind ABC is to generate simulated data that match the
real data \emph{exactly} but in the space of sufficient statistics.
In this problem, the empirical sample variance $s^2_y$ is a sufficient
statistic for the variance, so we are going to generate simulated
data, and find simulations that produce data that match our empirical
variance (which happens to be $\samplevar$).
In its simplest form (and there are many more complicated forms), ABC
operates by taking samples from the prior (the prior pdf for $\mu,
V$), generating fake data, and then rejecting the samples if the fake
data are not ``close enough'' to the real data.
To test ``close enough'', we compute a distance metric\footnote{There
  are constraints on what this distance metric can be, described in
  HOGG.} and compare it to a threshold.
For a distance metric $\Delta^2$, we use something very like the
pseudo-likelihood (\ref{eq:pseudo}):
\begin{eqnarray}
\Delta^2 &=& \frac{[s^2_y - \hat{s}^2_y]^2}{\Sigma^2}
\label{eq:dist}\quad ,
\end{eqnarray}
where $\hat{s}^2_y$ is the empirical sample variance we get for a set
of $N=5$ random draws from a gaussian with the prior-drawn parameters
$\mu, V$, and we have kept $\Sigma^2$ as-is.
When the distance threshold is set very large, ABC returns a prior
sampling.
In the limit that the distance threshold is very small---provided that
the distance metric compares all sufficient statistics in a convex
scalar combination---ABC returns a posterior sampling.

\figurename~\ref{fig:abc} shows the ABC-generated posterior sampling
for a small distance threshold (set to 1/16 in this case).
The distance metric $\Delta^2$ in (\ref{eq:dist}) makes no reference
to the empirical mean, so it doesn't constrain the true pdf mean
$\mu$.
However, the ABC results for the variance $V$ are indistinguishable
from the true posterior sampling shown in \figurename~\ref{fig:correct}.
The posterior sampling shown in \figurename~\ref{fig:abc} was made
only with prior pdf draws and the distance metric $\Delta^2$.
No real likelihood function was harmed.%
\begin{figure}
\includegraphics{abc_0005_-4.png}
\caption{Same as \figurename~\ref{fig:correct} but now with the output
  of the likelihood-free inference or ABC. Like in
  \figurename~\ref{fig:pseudo}, the ABC distance metric does not
  involve the empirical sample mean, so the mean parameter $\mu$
  remains unconstrained and the marginalized posterior pdf matches the
  prior pdf for this parameter. The posterior on the variance $V$
  looks very similar to the correct answer shown in
  \figurename~\ref{fig:correct}. This demonstrates that ABC can be
  used to draw correct inferences.\label{fig:abc}}
\end{figure}

\textbf{Take-home message 2:} \emph{If we use a variant of the
  pseudo-likelihood as a distance metric for an ABC, we can get
  correct posterior inferences without ever writing down a correct
  likelihood function.}

HOGG: Note that this version of ABC is idiotic.

HOGG: This might be tractable for LSS...

HOGG: Return to the LSS problem and discuss four things things: What
the RTTD is. What is the equivalent of the pseudo-likelihood
(\ref{eq:pseudo})? How, in detail, cosmologists get the $\Sigma$
horror. What ABC looks like. CITE HAHN AND VAKILI.

HOGG: ABC IS JUST ONE OPTION; discuss other general kinds of options.
They are vapor-ware!

\textbf{Take-home message 3:} \emph{There are very likely to be
  tractable versions of the ideas in this paper for large-scale
  structure inferences.}
Indeed, they might end up requiring less or comparable to current
inferences with the pseudo-likelihood, for the simple reason that,
right now, large numbers of simulations are required just to construct
the covariance matrix $\Sigma$.
HOGG: Check this...

HOGG: Why might all this be tractable?

\acknowledgements
It is a pleasure to thank
  Alex~Barnett (SCDA),
  Brendon~J.~Brewer (Auckland),
  Chang~Hoon~Hahn (NYU),
  Boris~Leistedt (NYU),
  Dan~Foreman-Mackey (UW),
  Jeremy~Tinker (NYU),
  M.~J.~Vakili (NYU),
  and the Blanton--Hogg group meeting at NYU
for valuable discussions.
This work was supported by the NSF (HOGG) and NASA (HOGG).
This project made use of the NASA Astrophysics Data System
and open-source software Python, numpy, and matplotlib.

\begin{thebibliography}{24}
\bibitem[Eisenstein \etal(2005)]{eisenstein}
  Eisenstein, D.~J., Zehavi, I., Hogg, D.~W., \etal, 2005, \apj, 633, 560 
\bibitem[Percival \etal(2014)]{percival}
  Percival, W.~J., Ross, A.~J., S{\'a}nchez, A.~G., \etal, 2014, \mnras, 439, 2531
\end{thebibliography}

\end{document}
