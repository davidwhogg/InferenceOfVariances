% This document is part of the InferenceOfVariance project.
% Copyright 2016 the author(s).

% To-do
% - be more specific about HOW the posterior is wrong.
% - add in material about sigma_8 being biased low!!
% - fix words about ``using the pseudo-likelihood'' as a distance function -- something LIKE it.
% - include the ABC description and results.
% - finish first draft.
% - cite Percival and Eisenstein. Be self-deprecating.
% - typeset projects properly (see acks)
% - new AAS \software tags
% - search for all occurrences of HOGG, DWH, or all-caps

\documentclass[12pt, letterpaper, preprint]{aastex}
\usepackage{hyperref}
\input{vc}
\input{data_0005}

% typesetting shih
\linespread{1.08} % close to 10/13 spacing
\setlength{\parindent}{1.08\baselineskip} % Bringhurst
\setlength{\parskip}{0ex}
\let\oldbibliography\thebibliography % killin' me.
\renewcommand{\thebibliography}[1]{%
  \oldbibliography{#1}%
  \setlength{\itemsep}{0pt}%
  \setlength{\parsep}{0pt}%
  \setlength{\parskip}{0pt}%
  \setlength{\bibsep}{0ex}
  \raggedright
}
\setlength{\footnotesep}{0ex} % seriously?

% math shih
\newcommand{\setof}[1]{\left\{{#1}\right\}}
\newcommand{\given}{\,|\,}
\newcommand{\pseudo}{{\mathrm{pseudo}}}
\newcommand{\Var}{\mathrm{Var}}

% text shih
\newcommand{\foreign}[1]{\textsl{#1}}
\newcommand{\etal}{\foreign{et~al.}}
\newcommand{\opcit}{\foreign{Op.~cit.}}
\newcommand{\documentname}{\textsl{Article}}
\newcommand{\equationname}{equation}

\begin{document}\sloppy\sloppypar\frenchspacing % oh yeah

\title{Inference of variances:\\
       Implications for cosmological large-scale structure experiments}
\author{
  David~W.~Hogg\altaffilmark{1,2}
}
\affil{Simons Center for Data Analysis,
       New York}
\altaffiltext{1}{also: Center for Cosmology and Particle Physics,
                       Department of Physics,
                       New York University}
\altaffiltext{2}{also: Center for Data Science,
                       New York University}
\email{david.hogg@nyu.edu}

\begin{abstract}
The study of large-scale structure is (at present) primarily the study
of variances, since the power spectrum or correlation function are
two-point statistics.
Present-day large-scale structure experiments often perform
cosmological inferences using a pseudo-likelihood that compares
(by something like chi-squared) a
point estimate of the two-point function to an equivalent estimate
from a simulation or theory.
Here we show---with toy problems---that this procedure in general
produces incorrect posterior inferences about the variance (and
therefore cosmological parameters).
However, we also show that if the same pseudo-likelihood
(or even something simpler) is used
\emph{not} as a likelihood in a standard probabilistic inference but
rather as a \emph{distance metric} in a likelihood-free inference or
approximate Bayesian
computation (ABC) method, correct posterior inferences can be drawn.
We discuss the relationships between the toy problems shown here and
the standard cosmological inferences, and try to suggest places where
a change from the standard incorrect method to a new method would
have the biggest impacts.
We argue that this is likely to be important at the baryon acoustic
scale and larger.
We emphasize that ABC is just one of many possible replacements for
the pseudo-likelihood method.
\end{abstract}

\keywords{
methods: data analysis
---
methods: statistical
---
galaxies: statistics
---
cosmological parameters
---
cosmology: observations
---
large-scale structure of universe
}

Present-day large-scale structure experiments are fundamentally
projects to measure the variance of cosmological fields, such as the
galaxy, quasar, intergalactic medium, or mass fields.
Indeed, if the initial conditions of the Universe are very close to
Gaussian (and they are) and if the evolution of the Universe is very
close to linear (as it is on large scales), then the mean and variance
of the density field (on some scale) are sufficient statistics to
infer everything about the initial conditions (at that scale).
Thus, the measurement or inference of variance as a function of
scale---or, equivalently, inference of the power spectrum or the
two-point auto-correlation function---is critical to inference of the
cosmological initial conditions and the cosmological parameters.
There is an enormous literature on making these cosmological
measurements!
Here we won't address this literature in detail.
We are going to step back and look at issues of principle in measuring
variances, and see if there are small changes to be made to
present-day cosmological practice that might improve future cosmological
inferences.

Imagine \emph{the most trivial variance estimation problem possible}:
You have been given $N$ samples $y_n$ drawn fairly and independently (iid)
from a Gaussian probability density function (hereafter ``pdf'').
What is the mean $\mu$ and variance $V$ of the pdf from which they
were drawn?
There is a classical answer that the empirial mean and empirical
variance (or sample mean and sample variance) of the sampling
\begin{eqnarray}
\bar{y} &\equiv& \frac{1}{N}\,\sum_{n=1}^N y_n
\label{eq:empmean}\\
s^2_y &\equiv& \frac{1}{N-1}\,\sum_{n=1}^N [y_n - \bar{y}]^2
\label{eq:empvar}
\end{eqnarray}
(with possibly some tiny adjustments) are good
\emph{estimators}\footnote{There is a little bit of confusion in the
  cosmological literature between the variance of a distribution and
  the \emph{empirical variance of a sampling} of that distribution. In this
  \documentname, the \emph{mean} and \emph{variance} are properties of
  the pdf from which the data were generated. The \emph{empirical
    mean} and the \emph{empirical variance} are things you get by
  doing arithmetical operations like \equationname
  s~(\ref{eq:empmean}) and (\ref{eq:empvar}) on the data.  This
  distinction is important for cosmology, because sometimes there are
  statements or implications that the cosmological parameters are
  being inferred from the variance of the density field. They are not!
  They are being inferred from an \emph{empirical estimate} of the
  variance of the density field---a sample variance. And while these empirical estimates
  will be sufficient statistics for the variance,
  this is only relevant and valuable if
  they are used \emph{correctly}.} for the mean and variance of the
pdf from which the $y_n$ were drawn.
That's cool, but what if we want full posterior information about the
variance $V$?
In contemporary cosmological experiments we \emph{always} want full
posterior (or at least likelihood) information about the parameters.
That is, the cosmological inference community is (mostly) Bayesian,
so in what follows we will produce Bayesian outputs, which are (for
our purposes) likelihood functions, posterior pdfs, and posterior
samplings.

It will be important for what follows that while equations
(\ref{eq:empmean}) and (\ref{eq:empvar}) deliver only empirical
point estimates of the mean and variance of the pdf we care about,
they \emph{are} sufficient statistics for the inference!
That is, \emph{if we believe that the $y_n$ were drawn iid from a
  Gaussian}, it is possible to write down a correct likelihood
function for the set $\setof{y_n}_{n=1}^N$ using only these two
summary statistics.
As we will see, however, it is also possible to write down an
incorrect likelihood function using them!

The (Bayesian) Right Thing To Do is to write down our beliefs about
how the data were generated, and then derive (analytically or
numerically) the posterior pdf\footnote{There will be several possible
  points of confusion in this \documentname. One is that there is a
  pdf for the data $y_n$. And there is also a pdf for the parameters
  $(\mu,V)$. These are different pdfs that live in different
  spaces. Another is that the data $y_n$ were drawn from a Gaussian,
  while the parameter $V$ will (in general) not have a
  posterior pdf  or likelihood function that is Gaussian in shape. That is, even if the data are
  generated by a Gaussian, there is no sense that the likelihood
  function will be Gaussian.} for the mean and
variance.
This, however, is \emph{not what's typically done in a cosmological
experiment!}
The reason is simple: It is difficult or perhaps impossible to compute
the likelihood in the cosmological context: It would require a pdf
over all possible experimental outcomes, which means all possible
positions of all possible galaxies in the survey volume.\footnote{We
  believe that there might be a tractable likelihood function for at
  least some cosmological experiments. We are working on testing this
  now. (Don't hold your breath.)}
What's done instead is to compare the empirical variance (the
correlation function or the power spectrum) of the observed data to
the variance observed in mock data or a simulation, and find
parameters that generate simulations that generate similar observed
variances.
There is nothing wrong with this procedure in general.  There are a
large family of methods by which this comparison could be performed
that would generate correct inferences about the cosmological
parameters.
However, the cosmological community has not (generally) used any of
these methods.

Now in this trivial problem---infer the variance $V$ given sampling
$\setof{y_n}_{n=1}^N$---one approach one could take to developing full
posterior inferences about $V$ would be the following:
Since the empirical variance $s^2_y$ defined in (\ref{eq:empvar})
is a sufficient statistic for the process variance $V$, \emph{make up}
a \emph{pseudo-likelihood} $p_\pseudo(s^2_y\given V)$ of the form
\begin{eqnarray}
\ln p_\pseudo(s^2_y\given V) &=& -\frac{1}{2}\,\frac{[s^2_y - V]^2}{\Sigma^2}
\label{eq:pseudo}\quad ,
\end{eqnarray}
where $\Sigma^2$ is a variance on the variance, determined somehow (to
be discussed more below).
This pseudo-likelihood gets the prefix ``pseudo'' because it was
created by an ansatz, and not by writing down our beliefs about the
process by which the samples were generated.
It gets the suffix ``likelihood'' because it is a (made up)
probability for the data given parameters, where in this case
$s^2_y$ is a sufficient statistic of the data, and $V$ is a
parameter of the unknown pdf from which the samples were drawn.
Note the Bayesianism inherent here:
The \emph{likelihood} is going to be the critical component of the
inference; the likelihood is based on \emph{beliefs} about the
data-generation process; and the parameter $V$ of the pdf that
generates the data is unknown (and, essentially, unknowable).

This made-up pseudo-likelihood (\ref{eq:pseudo}) is patently absurd:
For one, it does not represent anyone's beliefs about the data generation.
For another, it has no pathology or odd behavior when the variance $V$
goes to zero or negative!
That is, it doesn't even respect the basic properties of variances.
And yet, this is \emph{precisely analogous to the standard practice in
  large-scale structure}.

Since it is such a killing argument in the trivial problem, let's
reflect for a moment on how this non-negativity point comes in to the
real problem of large-scale structure inferences.
What is the equivalent property to non-negativity for the correlation
function (for, say, galaxies, the temperature map of the cosmic
microwave background, or the initial conditions of the density field)?
The answer is slightly non-trivial:
It is that the correlation function (what ought to be called the
``covariance function'') be a non-negative semi-definite function, or
that the correlation function obey what's called Mercer's condition\footnote{\url{https://en.wikipedia.org/wiki/Mercer\%27s_condition}}
or that any covariance matrix made up by any evaluation of the
correlation function on any grid of points be non-negative
semi-definite.
These (identical) conditions are best explained with equations.

First, define the correlation function $\xi(|r|)$ as the covariance of
the overdensity field $\delta(x)$, where $r$ is a vector displacement
in three-space (and $|r|$ is a scalar separation) and $x$ is
three-dimensional vector position:
\begin{eqnarray}
\xi(|r|) &=& E[\delta(x)\,\delta(x+r)]
\\
\delta(x) &\equiv& \frac{\rho(x)}{\bar{\rho}} - 1
\quad ,
\end{eqnarray}
where $E[q]$ is the expectation value of $q$ (implicitly taken as an integral
over all possible draws of the density field $\rho(x)$ and over all space in each of those draws),
$\rho$ is the density field,
and $\bar{\rho}$ is the mean density.
By assuming that $\xi(\cdot)$ depends only on $|r|$ we are effectively
assuming statistical isotropy.
Now choose a spatial set of points $\setof{x_i}$ in three-dimensional space
and construct the matrix $C$ such that the elements $C_{ij}$ of $C$
are given by
\begin{eqnarray}
C_{ij} &\equiv& \xi(|x_i - x_j|)
\quad .
\end{eqnarray}
This matrix is manifestly symmetric, but if $\xi(|r|)$ is the true
covariance function of the density field, then this matrix must also
be non-negative semi-definite.
That is, it must have eigenvalues that are real and non-negative.
That this be true for any choice of points $\setof{x_i}$ is equivalent
(for our purposes) to Mercer's condition.
That is, there is no requirement that the correlation function (really
covariance function) be positive everywhere; it can go negative!
But it must be such that it always produces non-negative semi-definite
covariance tensors.\footnote{This has an important consequence for
  inferences we do in cosmology: \emph{When we infer the correlation
    function (what should be called the covariance function) of the
    density field, let's only consider functions that obey Mercer's
    condition!}  (After all, we would never consider $V<0$ in the
  trivial problem!)  This is much easier to do in Fourier space than
  real space.  That isn't an argument for doing the \emph{analysis} in
  Fourier space, but it is an argument for representing the function
  $\xi(|r|)$ in Fourier space. This is off-topic for the present
  \documentname, of course, since this is a point about the
  \emph{prior} and not about the \emph{likelihood}. But somehow it
  feels relevant. Of course if the inference is being done of the
  cosmological parameters (and not $\xi(|r|)$ itself), then only
  functions obeying Mercer's condition are under consideration,
  because the theory itself can only produce non-negative
  semi-definite functions.}

Now, what, exactly is the Right Thing To Do in the trivial problem?
It is very straightforward:
We write down the probability for each point, and product these
(or sum them in the log).
For iid $y_n$ drawn from a Gaussian, this looks like this:
\begin{eqnarray}
p(\setof{y_n}_{n=1}^N\given \mu,V) &=& -\frac{1}{2}\,\sum_{n=1}^N \frac{[y_n - \mu]^2}{V} - \frac{N}{2}\,\ln V
\label{eq:truelf}\quad ,
\end{eqnarray}
which is obtained by taking the ln of a Gaussian function.
It is left as an exercise to the reader to show that this likelihood
can be expressed in terms of (only) the two sufficient statistics given
in equations~(\ref{eq:empmean}) and (\ref{eq:empvar}).\footnote{Hint:
  Equation~(\ref{eq:truelf}) is quadratic in the data.}
Below, we will discuss the Right Thing To Do in the large-scale
structure problem. It's hard!

In order to test these ideas quantitatively, we need fake data.
Here we choose (arbitrarily) a True\footnote{Because we are making
  fake data, we have access to God's Truth (with a capital
  ``T''). This is not the case for real astronomical measurements!}
mean $\mu=7.0$ and True variance $V=17.0$ (units arbitrary).
With code associated with this \documentname\footnote{The calculations
  and \figurename s in this \documentname\ were generated from the code in
  repository \giturl\ (with hash \texttt{\githash~\gitdate}). You can
  clone this and run the code yourself!}, we generate $N=5$ samples
$y_n$.
These samples (and their empirical moments) are
\begin{eqnarray}
\setof{y_n}_{n=1}^N &=& \setof{\samples}
\\
\bar{y} &=& \samplemean
\\
s^2_y &=& \samplevar
\quad .
\end{eqnarray}

Now imagine (without questioning) that our prior pdf for the
parameters $\mu, V$ are uniform in the intervals $0<\mu<10$ and
$0<V<100$.
At this stage, we have two options for inference: Use the correct
likelihood (\ref{eq:truelf}) or else the pseudo-likelihood
(\ref{eq:pseudo}).
We do both, running a standard (but home-built) Metropolis--Hastings
MCMC code for $524288$ steps, and thinning the chain by a factor of
$16$.
A Gaussian in each parameter $\mu,V$ was employed as a proposal
distribution, and step sizes were set to give reasonable acceptance.
The details are all visible in the code\footnote{\opcit} associated
with this \documentname.
The results are shown in \figurename~\ref{fig:correct} and
\figurename~\ref{fig:pseudo}.%
\begin{figure}
\includegraphics{correct_0005.png}
\caption{HOGG Foo.\label{fig:correct}}
\end{figure}%
\begin{figure}
\includegraphics{pseudo_0005.png}
\caption{Same as \figurename~\ref{fig:correct} but with the likelihood
  replaced with the pseudo-likelihood (\ref{eq:pseudo}). The
  pseudo-likelihood does not involve the mean $\mu$, so it remains
  unconstrained and the marginalized posterior pdf matches the prior
  pdf for this parameter. The posterior on the variance $V$ looks very
  different from the correct answer shown in
  \figurename~\ref{fig:correct}.\label{fig:pseudo}}
\end{figure}

Two comments on these results:
First, the two inferences deliver very different results for the
variance $V$. This is the most important point.
In the pseudo-likelihood inference (\figurename~\ref{fig:pseudo}), the
pdf mean $\mu$ is unconstrained. This is because $\mu$ doesn't appear
at all in the pseudo-likelihood! But the key point is that the
posterior inferences about $V$ are very different, especially at very
small and very large values of $V$.

\textbf{Take-home message 1:} \emph{The standard practice in
  cosmology, even when applied to an extremely trivial problem,
  delivers incorrect inferences.}

Second, the use of the pseudo-likelihood (\ref{eq:pseudo}) required
the setting of the variance-of-variance parameter $\Sigma^2$.
To set this, we made use of theory\footnote{\url{https://en.wikipedia.org/wiki/Variance}}
about the variance $\Var[s^2_y]$ of
the empirical sample variance $s^2_y$.
\begin{eqnarray}
\Var[s^2_y] &=& \frac{1}{N}\,\left[\mu_4 - \frac{N-3}{N-1}\,V^2\right]
\quad ,
\end{eqnarray}
where $\mu_4$ is the (true, not estimated) fourth moment of the pdf $p(y)$.
For a Gaussian pdf this becomes
\begin{eqnarray}
\Var[s^2_y] &=& \frac{2\,V^2}{N-1}
\quad .
\end{eqnarray}
For these reasons, we set the pseudo-likelihood parameter $\Sigma^2$ to
\begin{eqnarray}
\Sigma^2 &\leftarrow& \frac{2\,[s^2_y]^2}{N-1}
\quad ,
\end{eqnarray}
where we are not being conservative because, for one, we are imagining
that $s^2_y$ is a good estimator for $V$, and, for two, we are
assuming implicitly that the true distribution has vanishing kurtosis
(is nearly Gaussian).
Although this choice is not conservative, it maps directly onto the
practice in large-scale structure studies, which is to set the
equivalent parameter using simulations of the data-generating process
with parameters set to something estimated from the data.
That is, the pseudo-likelihoods used in cosmology are not conservative
in exactly this sense.
It is important to note, however, that there is no choice that we
could make for $\Sigma^2$ such that our posterior inferences would e
correct given this form for the pseudo-likelihood; that is, the method
is not wrong because it is not conservative. It is wrong because it
is using an unreasonable likelihood function.

HOGG: Introduce ABC and compare to the previous two inferences.

\textbf{Take-home message 2:} \emph{If we use the pseudo-likelihood as
  a distance function in ABC, we can get correct posterior inferences
  without ever writing down a correct likelihood function.}
HOGG: This might be tractable for LSS...

HOGG: Return to the LSS problem and discuss four things things: What
the RTTD is. What is the equivalent of the pseudo-likelihood
(\ref{eq:pseudo})? How, in detail, cosmologists get the $\Sigma$
horror. What ABC looks like. CITE HAHN AND VAKILI.

HOGG: ABC IS JUST ONE OPTION; discuss other general kinds of options.
They are vapor-ware!

\textbf{Take-home message 3:} \emph{There are very likely to be
  tractable versions of the ideas in this paper for large-scale
  structure inferences.}
Indeed, they might end up requiring less or comparable to current
inferences with the pseudo-likelihood, for the simple reason that,
right now, large numbers of simulations are required just to construct
the covariance matrix $\Sigma$.
HOGG: Check this...

HOGG: Why might all this be tractable?

\acknowledgements
It is a pleasure to thank
  Brendon~J.~Brewer (Auckland),
  Chang~Hoon~Hahn (NYU),
  Dan~Foreman-Mackey (UW),
  Jeremy~Tinker (NYU),
  M.~J.~Vakili (NYU),
  and the Blanton--Hogg group meeting at NYU
for valuable discussions.
This work was supported by the NSF (HOGG) and NASA (HOGG).
This project made use of the NASA Astrophysics Data System
and open-source software Python, numpy, and matplotlib.

\begin{thebibliography}{24}
\bibitem[Eisenstein \etal(2005)]{eisenstein}
  Eisenstein, D.~J., Zehavi, I., Hogg, D.~W., \etal, 2005, \apj, 633, 560 
\bibitem[Percival \etal(2014)]{percival}
  Percival, W.~J., Ross, A.~J., S{\'a}nchez, A.~G., \etal, 2014, \mnras, 439, 2531
\end{thebibliography}

\end{document}
